"""
ç®€åŒ–çš„é…ç½®åŠ è½½å™¨
ç›´æ¥ä» YAML æ–‡ä»¶è¯»å–é…ç½®ï¼Œæ— éœ€å¤æ‚çš„ç±»ç»“æ„
"""

import yaml
from pathlib import Path
from typing import Dict, Any

def load_training_config(config_file: str = None) -> Dict[str, Any]:
    """åŠ è½½è®­ç»ƒé…ç½®
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º config/training.yaml
        
    Returns:
        é…ç½®å­—å…¸
    """
    if config_file is None:
        config_file = Path(__file__).parent / "training.yaml"
    
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # åº”ç”¨é¢„è®¾é…ç½®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if 'presets' in config and config['presets']:
            for preset_name, preset_config in config['presets'].items():
                if preset_config and isinstance(preset_config, dict):  # å¦‚æœé¢„è®¾ä¸ä¸ºç©ºä¸”æ˜¯å­—å…¸ï¼Œåº”ç”¨å®ƒ
                    print(f"ğŸ¯ åº”ç”¨é¢„è®¾é…ç½®: {preset_name}")
                    config['training'].update(preset_config)
                    break  # åªåº”ç”¨ç¬¬ä¸€ä¸ªéç©ºé¢„è®¾
        
        return config
        
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_file}")
        print("ä½¿ç”¨é»˜è®¤é…ç½®...")
        return get_default_config()
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        print("ä½¿ç”¨é»˜è®¤é…ç½®...")
        return get_default_config()

def get_default_config() -> Dict[str, Any]:
    """è·å–é»˜è®¤é…ç½®"""
    return {
        'model': {
            'name': 'xlm-roberta-base',
            'max_length': 512,
            'num_labels': 2
        },
        'training': {
            'batch_size': 8,
            'learning_rate': 1e-5,
            'num_epochs': 3,
            'weight_decay': 0.1,
            'max_grad_norm': 1.0,
            'label_smoothing_factor': 0.1,
            'early_stopping_patience': 2,
            'early_stopping_threshold': 0.0005,
            'warmup_steps': 100,
            'warmup_ratio': 0.1,
            'lr_scheduler_type': 'cosine_with_restarts',
            'gradient_accumulation_steps': 2,
            'gradient_checkpointing': True,
            'eval_strategy': 'steps',
            'eval_steps': 50,
            'save_strategy': 'steps',
            'save_steps': 100,
            'logging_steps': 25,
            'load_best_model_at_end': True,
            'metric_for_best_model': 'eval_loss',
            'greater_is_better': False,
            'dataloader_pin_memory': True,
            'dataloader_drop_last': True,
            'remove_unused_columns': True,
            'seed': 42
        },
        'data': {
            'train_ratio': 0.7,
            'val_ratio': 0.1,
            'test_ratio': 0.2,
            'random_seed': 42
        }
    }

def show_config(config: Dict[str, Any] = None):
    """æ˜¾ç¤ºå½“å‰é…ç½®"""
    if config is None:
        config = load_training_config()
    
    print("ğŸ¯ å½“å‰è®­ç»ƒé…ç½®:")
    print("="*50)
    
    model_config = config['model']
    training_config = config['training']
    
    print(f"ğŸ“š æ¨¡å‹é…ç½®:")
    print(f"   â€¢ æ¨¡å‹åç§°: {model_config['name']}")
    print(f"   â€¢ æœ€å¤§é•¿åº¦: {model_config['max_length']}")
    print(f"   â€¢ æ ‡ç­¾æ•°é‡: {model_config['num_labels']}")
    
    print(f"\nğŸ¯ è®­ç»ƒé…ç½®:")
    key_params = [
        ('batch_size', 'æ‰¹æ¬¡å¤§å°'),
        ('learning_rate', 'å­¦ä¹ ç‡'),
        ('num_epochs', 'è®­ç»ƒè½®æ•°'),
        ('weight_decay', 'æƒé‡è¡°å‡'),
        ('early_stopping_patience', 'æ—©åœè€å¿ƒå€¼'),
        ('early_stopping_threshold', 'æ—©åœé˜ˆå€¼'),
        ('label_smoothing_factor', 'æ ‡ç­¾å¹³æ»‘'),
        ('max_grad_norm', 'æ¢¯åº¦è£å‰ª'),
        ('lr_scheduler_type', 'å­¦ä¹ ç‡è°ƒåº¦å™¨')
    ]
    
    for key, desc in key_params:
        value = training_config.get(key, 'æœªè®¾ç½®')
        print(f"   â€¢ {desc}: {value}")
    
    print("="*50)

if __name__ == '__main__':
    # æµ‹è¯•é…ç½®åŠ è½½
    config = load_training_config()
    show_config(config)

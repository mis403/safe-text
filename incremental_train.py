#!/usr/bin/env python3
"""
å¢é‡è®­ç»ƒè„šæœ¬ - åœ¨å·²æœ‰æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒ
"""

import sys
import argparse
from pathlib import Path
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.models.trainer import SensitiveWordTrainer
from src.data import DataProcessor
from config.settings import config
from src.utils.logger import setup_logger
from src.utils.model_finder import find_latest_model

logger = setup_logger(__name__)

def merge_training_data(original_file: str, additional_file: str, output_file: str):
    """
    åˆå¹¶åŸå§‹è®­ç»ƒæ•°æ®å’Œæ–°å¢æ•°æ®
    
    Args:
        original_file: åŸå§‹è®­ç»ƒæ•°æ®æ–‡ä»¶
        additional_file: æ–°å¢è®­ç»ƒæ•°æ®æ–‡ä»¶
        output_file: åˆå¹¶åçš„è¾“å‡ºæ–‡ä»¶
    """
    logger.info(f"åˆå¹¶è®­ç»ƒæ•°æ®: {original_file} + {additional_file} -> {output_file}")
    
    # è¯»å–åŸå§‹æ•°æ®
    df_original = pd.read_csv(original_file)
    logger.info(f"åŸå§‹æ•°æ®: {len(df_original)} æ¡")
    
    # è¯»å–æ–°å¢æ•°æ®
    df_additional = pd.read_csv(additional_file)
    logger.info(f"æ–°å¢æ•°æ®: {len(df_additional)} æ¡")
    
    # åˆå¹¶æ•°æ®
    df_merged = pd.concat([df_original, df_additional], ignore_index=True)
    
    # å»é‡ï¼ˆåŸºäºtextåˆ—ï¼‰
    initial_count = len(df_merged)
    df_merged.drop_duplicates(subset=['text'], inplace=True)
    final_count = len(df_merged)
    
    logger.info(f"åˆå¹¶åæ•°æ®: {initial_count} æ¡")
    logger.info(f"å»é‡åæ•°æ®: {final_count} æ¡")
    logger.info(f"å»é™¤é‡å¤: {initial_count - final_count} æ¡")
    
    # ä¿å­˜åˆå¹¶åçš„æ•°æ®
    df_merged.to_csv(output_file, index=False)
    logger.info(f"åˆå¹¶æ•°æ®ä¿å­˜åˆ°: {output_file}")
    
    return output_file

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¢é‡è®­ç»ƒæ•æ„Ÿè¯æ£€æµ‹æ¨¡å‹")
    parser.add_argument('--additional-data', type=str, required=True, help='æ–°å¢è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--base-model', type=str, help='åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆä¸æŒ‡å®šåˆ™è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹ï¼‰')
    parser.add_argument('--original-data', type=str, default='data/formatted_sensitive_data.csv', help='åŸå§‹è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', type=str, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--learning-rate', type=float, default=1e-5, help='å­¦ä¹ ç‡ï¼ˆå¢é‡è®­ç»ƒå»ºè®®ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼‰')
    parser.add_argument('--epochs', type=int, default=2, help='è®­ç»ƒè½®æ•°ï¼ˆå¢é‡è®­ç»ƒå»ºè®®ä½¿ç”¨è¾ƒå°‘è½®æ•°ï¼‰')
    
    args = parser.parse_args()
    
    try:
        logger.info("å¼€å§‹å¢é‡è®­ç»ƒ...")
        
        # æ£€æŸ¥æ–°å¢æ•°æ®æ–‡ä»¶
        if not Path(args.additional_data).exists():
            raise FileNotFoundError(f"æ–°å¢è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.additional_data}")
        
        # æ£€æŸ¥åŸå§‹æ•°æ®æ–‡ä»¶
        if not Path(args.original_data).exists():
            raise FileNotFoundError(f"åŸå§‹è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.original_data}")
        
        # åˆå¹¶è®­ç»ƒæ•°æ®
        merged_data_file = "data/merged_training_data.csv"
        merge_training_data(args.original_data, args.additional_data, merged_data_file)
        
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        processor = DataProcessor()
        
        # å¤„ç†åˆå¹¶åçš„æ•°æ®
        logger.info(f"å¤„ç†åˆå¹¶åçš„æ•°æ®: {merged_data_file}")
        train_file = config.data_config["train_file"]
        val_file = config.data_config["val_file"]
        test_file = config.data_config["test_file"]
        
        processor.process_and_split(merged_data_file, train_file, val_file, test_file)
        
        # æŸ¥æ‰¾åŸºç¡€æ¨¡å‹
        if args.base_model:
            base_model_path = args.base_model
        else:
            base_model_path = find_latest_model()
            if not base_model_path:
                logger.warning("æœªæ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹ï¼Œå°†ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹è®­ç»ƒ")
                base_model_path = "xlm-roberta-base"
        
        logger.info(f"ä½¿ç”¨åŸºç¡€æ¨¡å‹: {base_model_path}")
        
        # åˆå§‹åŒ–è®­ç»ƒå™¨
        trainer = SensitiveWordTrainer(base_model_path)
        
        # è®¾ç½®è¾ƒå°çš„å­¦ä¹ ç‡ç”¨äºå¢é‡è®­ç»ƒ
        trainer.training_config.update({
            "learning_rate": args.learning_rate,
            "num_train_epochs": args.epochs,
            "warmup_steps": 100,  # å‡å°‘warmupæ­¥æ•°
        })
        
        logger.info(f"å¢é‡è®­ç»ƒé…ç½®: å­¦ä¹ ç‡={args.learning_rate}, è½®æ•°={args.epochs}")
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        trainer.load_model_and_tokenizer()
        
        # å‡†å¤‡æ•°æ®é›†
        train_dataset, val_dataset, test_dataset = trainer.prepare_datasets(
            train_file, val_file, test_file if Path(test_file).exists() else None
        )
        
        # è®¾ç½®è®­ç»ƒ
        output_dir = trainer.setup_training(train_dataset, val_dataset, args.output_dir)
        
        # è®­ç»ƒæ¨¡å‹
        results = trainer.train()
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        if test_dataset:
            test_results = trainer.evaluate_on_test_set(test_dataset)
            logger.info(f"æµ‹è¯•ç»“æœ: {test_results}")
        
        # ä¿å­˜æ¨¡å‹
        model_path = trainer.save_model()
        
        logger.info("å¢é‡è®­ç»ƒå®Œæˆ!")
        logger.info(f"æ¨¡å‹ä¿å­˜åˆ°: {model_path}")
        logger.info(f"è®­ç»ƒæ—¥å¿—ä¿å­˜åˆ°: {output_dir}")
        
        print(f"\nâœ… å¢é‡è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åˆ°: {model_path}")
        print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—: {output_dir}")
        
        if test_dataset:
            test_accuracy = test_results.get('accuracy', 0)
            test_f1 = test_results.get('f1', 0)
            print(f"ğŸ¯ æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
            print(f"ğŸ“ˆ æµ‹è¯•F1åˆ†æ•°: {test_f1:.4f}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if Path(merged_data_file).exists():
            Path(merged_data_file).unlink()
            logger.info(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {merged_data_file}")
        
    except Exception as e:
        logger.error(f"å¢é‡è®­ç»ƒå¤±è´¥: {e}")
        print(f"âŒ å¢é‡è®­ç»ƒå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()

#!/usr/bin/env python3
"""
æ•æ„Ÿè¯æ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬ - é›†æˆé˜²è¿‡æ‹ŸåˆåŠŸèƒ½
æ”¯æŒä»é›¶å¼€å§‹è®­ç»ƒå’Œå¢é‡è®­ç»ƒ
"""

import sys
import argparse
import shutil
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from config.simple_config import load_training_config, show_config
from src.models.trainer import SensitiveWordTrainer
from src.data import DataProcessor
from config.settings import config
from src.utils.logger import setup_logger
from src.utils.overfitting_detector import OverfittingDetector
import torch
import numpy as np
import random

logger = setup_logger(__name__)

def set_deterministic_training(seed=42):
    """è®¾ç½®ç¡®å®šæ€§è®­ç»ƒç¯å¢ƒ"""
    # Pythonéšæœºç§å­
    random.seed(seed)
    
    # NumPyéšæœºç§å­
    np.random.seed(seed)
    
    # PyTorchéšæœºç§å­
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    # è®¾ç½®ç¡®å®šæ€§ç®—æ³•ï¼ˆå¯èƒ½å½±å“æ€§èƒ½ï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    import os
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    logger.info(f"âœ… è®¾ç½®ç¡®å®šæ€§è®­ç»ƒç¯å¢ƒï¼Œç§å­: {seed}")

def clear_previous_models(keep_cache=True):
    """æ¸…ç†ä¹‹å‰çš„æ¨¡å‹æ–‡ä»¶ï¼Œä½†ä¿ç•™ç¼“å­˜"""
    models_dir = Path("models")
    
    # æ¸…ç†xlm_roberta_sensitive_filterç›®å½•
    model_filter_dir = models_dir / "xlm_roberta_sensitive_filter"
    if model_filter_dir.exists():
        logger.info(f"æ¸…ç†æ—§æ¨¡å‹: {model_filter_dir}")
        shutil.rmtree(model_filter_dir)
    
    # æ¸…ç†ultimate_xlm_roberta_modelç›®å½•  
    ultimate_model_dir = Path("ultimate_xlm_roberta_model")
    if ultimate_model_dir.exists():
        logger.info(f"æ¸…ç†æ—§æ¨¡å‹: {ultimate_model_dir}")
        shutil.rmtree(ultimate_model_dir)
    
    # æ¸…ç†è¾“å‡ºç›®å½•ï¼ˆä¿ç•™æœ€æ–°çš„ç”¨äºåˆ†æï¼‰
    outputs_dir = Path("outputs")
    if outputs_dir.exists():
        training_dirs = [d for d in outputs_dir.iterdir() if d.is_dir() and d.name.startswith('training_')]
        if len(training_dirs) > 2:  # ä¿ç•™æœ€è¿‘2ä¸ªè®­ç»ƒç›®å½•
            training_dirs.sort(key=lambda x: x.stat().st_mtime)
            for old_dir in training_dirs[:-2]:
                logger.info(f"æ¸…ç†æ—§è®­ç»ƒç›®å½•: {old_dir}")
                shutil.rmtree(old_dir)
    
    logger.info("âœ… æ¨¡å‹æ¸…ç†å®Œæˆ")

def prepare_training_data(input_data=None, force_resplit=False, use_random_split=False):
    """å‡†å¤‡å’ŒéªŒè¯è®­ç»ƒæ•°æ®
    
    Args:
        input_data: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„
        force_resplit: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ†å‰²æ•°æ®
        use_random_split: æ˜¯å¦ä½¿ç”¨éšæœºåˆ†å‰²ï¼ˆæ¯æ¬¡ä¸åŒçš„åˆ†å‰²ç»“æœï¼‰
    """
    # æ ¹æ®æ˜¯å¦ä½¿ç”¨éšæœºåˆ†å‰²æ¥åˆå§‹åŒ–å¤„ç†å™¨
    processor = DataProcessor(use_random_seed=use_random_split)
    
    # å¦‚æœæä¾›äº†è¾“å…¥æ•°æ®ï¼Œå…ˆå¤„ç†å®ƒ
    if input_data:
        if not Path(input_data).exists():
            raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {input_data}")
        
        logger.info(f"å¤„ç†è¾“å…¥æ•°æ®: {input_data}")
        if use_random_split:
            logger.info("ğŸ² ä½¿ç”¨éšæœºæ•°æ®åˆ†å‰² - æ¯æ¬¡è®­ç»ƒå°†ä½¿ç”¨ä¸åŒçš„æ•°æ®åˆ†å¸ƒ")
        else:
            logger.info("ğŸ”’ ä½¿ç”¨å›ºå®šæ•°æ®åˆ†å‰² - ç¡®ä¿å¯é‡ç°æ€§")
            
        train_file = "data/train.csv"
        val_file = "data/val.csv"
        test_file = "data/test.csv"
        
        processor.process_and_split(input_data, train_file, val_file, test_file)
        logger.info("âœ… æ•°æ®å¤„ç†å®Œæˆ")
    
    # æ£€æŸ¥æ ¼å¼åŒ–æ•°æ®æ˜¯å¦å­˜åœ¨ï¼ˆç”¨äºä»é›¶è®­ç»ƒï¼‰
    formatted_data = Path("data/formatted_sensitive_data.csv")
    train_file = Path("data/train.csv")
    val_file = Path("data/val.csv") 
    test_file = Path("data/test.csv")
    
    # å¦‚æœå¼ºåˆ¶é‡æ–°åˆ†å‰²æˆ–åˆ†å‰²æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ ¼å¼åŒ–æ•°æ®åˆ†å‰²
    if force_resplit or not all([train_file.exists(), val_file.exists(), test_file.exists()]):
        if formatted_data.exists():
            if force_resplit:
                logger.info("å¼ºåˆ¶é‡æ–°åˆ†å‰²è®­ç»ƒæ•°æ®...")
            else:
                logger.info("ä»æ ¼å¼åŒ–æ•°æ®åˆ†å‰²è®­ç»ƒæ•°æ®...")
            processor.process_and_split(
                str(formatted_data),
                str(train_file),
                str(val_file), 
                str(test_file)
            )
            logger.info("âœ… æ•°æ®åˆ†å‰²å®Œæˆ")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œè¯·æä¾› --input-data æˆ–ç¡®ä¿ data/formatted_sensitive_data.csv å­˜åœ¨")
    else:
        logger.info("âœ… ä½¿ç”¨ç°æœ‰çš„æ•°æ®åˆ†å‰²æ–‡ä»¶ï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰")
    
    # éªŒè¯æ•°æ®æ–‡ä»¶
    for file_path in [train_file, val_file, test_file]:
        if not file_path.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        size = file_path.stat().st_size
        if size < 1000:  # å°äº1KBå¯èƒ½æœ‰é—®é¢˜
            logger.warning(f"æ•°æ®æ–‡ä»¶å¯èƒ½è¿‡å°: {file_path} ({size} bytes)")
    
    logger.info(f"âœ… è®­ç»ƒæ•°æ®éªŒè¯å®Œæˆ")
    logger.info(f"   ğŸ“ è®­ç»ƒé›†: {train_file}")
    logger.info(f"   ğŸ“ éªŒè¯é›†: {val_file}")
    logger.info(f"   ğŸ“ æµ‹è¯•é›†: {test_file}")
    
    return str(train_file), str(val_file), str(test_file)

def print_training_config(config):
    """æ‰“å°è®­ç»ƒé…ç½®ä¿¡æ¯"""
    print("\nğŸ”§ è®­ç»ƒé…ç½®ä¿¡æ¯:")
    show_config(config)

def monitor_training_progress(output_dir):
    """ç›‘æ§è®­ç»ƒè¿›åº¦å’Œè¿‡æ‹Ÿåˆ"""
    detector = OverfittingDetector()
    
    try:
        # åˆ†æè®­ç»ƒæ—¥å¿—
        analysis = detector.analyze_training_logs(output_dir)
        
        if analysis.get("overfitting"):
            logger.warning("âš ï¸  æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆä¿¡å·:")
            for signal in analysis.get("signals", []):
                logger.warning(f"   â€¢ {signal}")
            
            if analysis.get("recommendation"):
                logger.info(f"ğŸ’¡ å»ºè®®: {analysis['recommendation']}")
        else:
            logger.info("âœ… æœªæ£€æµ‹åˆ°æ˜æ˜¾è¿‡æ‹Ÿåˆ")
        
        # ä¿å­˜åˆ†æå›¾è¡¨
        if 'data' in analysis:
            save_path = Path("outputs") / "training_analysis_latest.png"
            detector.visualize_training(analysis, str(save_path))
            logger.info(f"ğŸ“Š è®­ç»ƒåˆ†æå›¾ä¿å­˜åˆ°: {save_path}")
        
        return analysis
        
    except Exception as e:
        logger.error(f"è®­ç»ƒç›‘æ§å¤±è´¥: {e}")
        return {"error": str(e)}

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•æ„Ÿè¯æ£€æµ‹æ¨¡å‹è®­ç»ƒ - æ”¯æŒé˜²è¿‡æ‹Ÿåˆ")
    parser.add_argument('--input-data', type=str, help='è¾“å…¥è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config/training.yaml)')
    parser.add_argument('--model-name', type=str, help='é¢„è®­ç»ƒæ¨¡å‹åç§° (è¦†ç›–é…ç½®æ–‡ä»¶)')
    parser.add_argument('--output-dir', type=str, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--clear-models', action='store_true', help='æ¸…ç†ä¹‹å‰çš„æ¨¡å‹æ–‡ä»¶')
    parser.add_argument('--skip-monitoring', action='store_true', help='è·³è¿‡è¿‡æ‹Ÿåˆç›‘æ§')
    parser.add_argument('--simple-mode', action='store_true', help='ç®€åŒ–æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºè¯¦ç»†é…ç½®')
    parser.add_argument('--force-resplit', action='store_true', help='å¼ºåˆ¶é‡æ–°åˆ†å‰²æ•°æ®ï¼ˆå¯èƒ½å¯¼è‡´ä¸åŒç»“æœï¼‰')
    parser.add_argument('--random-split', action='store_true', help='ä½¿ç”¨éšæœºæ•°æ®åˆ†å‰²ï¼ˆæ¯æ¬¡è®­ç»ƒä¸åŒçš„æ•°æ®åˆ†å¸ƒï¼‰')
    parser.add_argument('--deterministic', action='store_true', help='å¯ç”¨å®Œå…¨ç¡®å®šæ€§è®­ç»ƒï¼ˆå¯èƒ½å½±å“æ€§èƒ½ï¼‰')
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½é…ç½®
        config = load_training_config(args.config)
        model_name = args.model_name or config['model']['name']
        
        # è®¾ç½®ç¡®å®šæ€§è®­ç»ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if args.deterministic:
            training_seed = config['training'].get('seed', 42)
            set_deterministic_training(training_seed)
        
        if not args.simple_mode:
            print("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ - é˜²è¿‡æ‹Ÿåˆç‰ˆæœ¬")
            print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print_training_config(config)
        else:
            logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # 1. æ¸…ç†æ—§æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if args.clear_models:
            clear_previous_models()
        
        # 2. å‡†å¤‡è®­ç»ƒæ•°æ®
        logger.info("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        train_file, val_file, test_file = prepare_training_data(
            args.input_data, 
            args.force_resplit, 
            args.random_split
        )
        
        # 3. åˆå§‹åŒ–è®­ç»ƒå™¨ï¼ˆä¼ å…¥é…ç½®ï¼‰
        logger.info(f"ğŸ¤– åˆå§‹åŒ–è®­ç»ƒå™¨ï¼Œæ¨¡å‹: {model_name}")
        trainer = SensitiveWordTrainer(model_name, config)
        
        # 4. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        logger.info("ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨...")
        trainer.load_model_and_tokenizer()
        
        # 5. å‡†å¤‡æ•°æ®é›†
        logger.info("ğŸ—ƒï¸  å‡†å¤‡æ•°æ®é›†...")
        train_dataset, val_dataset, test_dataset = trainer.prepare_datasets(
            train_file, val_file, test_file
        )
        
        logger.info(f"ğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡:")
        logger.info(f"   â€¢ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        logger.info(f"   â€¢ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        logger.info(f"   â€¢ æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        
        # 6. è®¾ç½®è®­ç»ƒ
        logger.info("âš™ï¸  è®¾ç½®è®­ç»ƒé…ç½®...")
        output_dir = trainer.setup_training(train_dataset, val_dataset, args.output_dir)
        
        # 7. å¼€å§‹è®­ç»ƒ
        logger.info("ğŸ¯ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        if not args.simple_mode:
            print("\n" + "="*60)
            print("ğŸ”¥ è®­ç»ƒè¿›è¡Œä¸­... è¯·ç­‰å¾…")
            print("ğŸ’¡ æç¤º: è®­ç»ƒå°†è‡ªåŠ¨åº”ç”¨æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ")
            print("="*60)
        
        results = trainer.train()
        logger.info("âœ… è®­ç»ƒå®Œæˆ!")
        
        # 8. ç›´æ¥åœ¨è®­ç»ƒç›®å½•ä¸­ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_dir = Path(output_dir) / "final_model"
        final_model_dir.mkdir(exist_ok=True)
        
        # ç›´æ¥ä¿å­˜åˆ°è®­ç»ƒç›®å½•
        logger.info(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°è®­ç»ƒç›®å½•: {final_model_dir}")
        trainer.model.save_pretrained(final_model_dir)
        trainer.tokenizer.save_pretrained(final_model_dir)
        
        # ä¿å­˜è®­ç»ƒé…ç½®
        training_config_dict = {
            'model_name': args.model_name,
            'model_config': trainer.model_config,
            'training_config': trainer.training_config,
            'device': trainer.device,
            'timestamp': datetime.now().isoformat()
        }
        
        import json
        with open(final_model_dir / 'training_config.json', 'w') as f:
            json.dump(training_config_dict, f, indent=2)
        
        # 9. æµ‹è¯•é›†è¯„ä¼°ï¼ˆä½¿ç”¨åˆšä¿å­˜çš„æ¨¡å‹ï¼‰
        test_results = None
        if test_dataset:
            logger.info("ğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
            try:
                # ä½¿ç”¨æ¨ç†æ¥å£è¿›è¡Œæµ‹è¯•é›†è¯„ä¼°
                from src.models.inference import SensitiveWordInference
                import pandas as pd
                from sklearn.metrics import accuracy_score
                
                # åŠ è½½åˆšä¿å­˜çš„æ¨¡å‹
                inference_model = SensitiveWordInference(str(final_model_dir))
                
                # åŠ è½½æµ‹è¯•æ•°æ®
                test_df = pd.read_csv(test_file)
                sample_size = min(200, len(test_df))  # æµ‹è¯•å‰200ä¸ªæ ·æœ¬
                test_sample = test_df.head(sample_size)
                
                predictions = []
                true_labels = test_sample['label'].tolist()
                
                for _, row in test_sample.iterrows():
                    try:
                        result = inference_model.predict_single(row['text'])
                        pred_label = 1 if result['is_sensitive'] else 0
                        predictions.append(pred_label)
                    except:
                        predictions.append(0)  # é»˜è®¤ä¸ºæ­£å¸¸
                
                # è®¡ç®—æŒ‡æ ‡
                accuracy = accuracy_score(true_labels, predictions)
                test_results = {
                    'accuracy': accuracy,
                    'sample_size': sample_size,
                    'total_size': len(test_df)
                }
                
                logger.info(f"æµ‹è¯•é›†è¯„ä¼°å®Œæˆ (æ ·æœ¬: {sample_size}/{len(test_df)})")
                logger.info(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
                
            except Exception as e:
                logger.warning(f"æµ‹è¯•é›†è¯„ä¼°å¤±è´¥: {e}")
        
        # ä¿å­˜è®­ç»ƒæ‘˜è¦
        training_summary = {
            'training_date': datetime.now().isoformat(),
            'model_name': args.model_name,
            'final_model_path': str(final_model_dir),
            'training_output_dir': output_dir,
            'test_results': test_results
        }
        
        with open(Path(output_dir) / "training_summary.json", 'w', encoding='utf-8') as f:
            json.dump(training_summary, f, indent=2, ensure_ascii=False)
        
        # 10. æ›´æ–°modelsç›®å½•çš„ç¬¦å·é“¾æ¥
        try:
            models_link = Path("models/xlm_roberta_sensitive_filter")
            if models_link.exists() or models_link.is_symlink():
                models_link.unlink()  # åˆ é™¤ç°æœ‰çš„æ–‡ä»¶æˆ–ç¬¦å·é“¾æ¥
            
            # åˆ›å»ºæ–°çš„ç¬¦å·é“¾æ¥æŒ‡å‘æœ€æ–°è®­ç»ƒç»“æœ
            relative_path = Path("..") / "outputs" / Path(output_dir).name / "final_model"
            models_link.symlink_to(relative_path)
            
            logger.info(f"âœ… æ›´æ–°ç¬¦å·é“¾æ¥: models/xlm_roberta_sensitive_filter -> {relative_path}")
            model_path = str(models_link)  # è¿”å›ç¬¦å·é“¾æ¥è·¯å¾„ä»¥ä¿æŒå…¼å®¹æ€§
            
        except Exception as e:
            logger.warning(f"ç¬¦å·é“¾æ¥æ›´æ–°å¤±è´¥: {e}")
            model_path = str(final_model_dir)  # è¿”å›å®é™…è·¯å¾„
        
        # 9. è¿‡æ‹Ÿåˆç›‘æ§å’Œåˆ†æ
        if not args.skip_monitoring:
            logger.info("ğŸ” åˆ†æè®­ç»ƒè¿‡ç¨‹å’Œè¿‡æ‹Ÿåˆ...")
            analysis = monitor_training_progress(output_dir)
            
            if not args.simple_mode:
                if analysis.get("overfitting"):
                    print(f"\nâš ï¸  è¿‡æ‹Ÿåˆæ£€æµ‹ç»“æœ: æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆä¿¡å·")
                    print(f"ğŸ’¡ å»ºè®®: {analysis.get('recommendation', 'æ— ')}")
                else:
                    print(f"\nâœ… è¿‡æ‹Ÿåˆæ£€æµ‹ç»“æœ: è®­ç»ƒè‰¯å¥½")
        
        # æœ€ç»ˆç»“æœæ±‡æ€»
        if not args.simple_mode:
            print("\n" + "="*60)
            print("ğŸ‰ è®­ç»ƒå®Œæˆ! ç»“æœæ±‡æ€»:")
            print("="*60)
            print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
            print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—ç›®å½•: {output_dir}")
            
            if test_results:
                print(f"ğŸ¯ æµ‹è¯•å‡†ç¡®ç‡: {test_results['accuracy']:.4f} (æ ·æœ¬: {test_results['sample_size']}/{test_results['total_size']})")
            
            analysis_plot = Path("outputs") / "training_analysis_latest.png"
            if analysis_plot.exists():
                print(f"ğŸ“Š è®­ç»ƒåˆ†æå›¾: {analysis_plot}")
            
            print(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("="*60)
        else:
            print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
            print(f"ğŸ“ æ¨¡å‹ä¿å­˜åˆ°: {model_path}")
            print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—: {output_dir}")
            
            if test_results:
                print(f"ğŸ¯ æµ‹è¯•å‡†ç¡®ç‡: {test_results['accuracy']:.4f}")
        
    except Exception as e:
        logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        
        # å¦‚æœæœ‰éƒ¨åˆ†ç»“æœï¼Œå°è¯•ä¿å­˜
        try:
            if 'trainer' in locals() and trainer.model:
                emergency_path = Path("emergency_model_save")
                trainer.save_model(str(emergency_path))
                print(f"ğŸš‘ ç´§æ€¥ä¿å­˜æ¨¡å‹åˆ°: {emergency_path}")
        except:
            pass
        
        sys.exit(1)

if __name__ == '__main__':
    main()
#!/usr/bin/env python3
"""
ç®€åŒ–çš„æ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.models.trainer import SensitiveWordTrainer
from src.data import DataProcessor
from config.settings import config
from src.utils.logger import setup_logger

logger = setup_logger(__name__)

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    parser = argparse.ArgumentParser(description="è®­ç»ƒæ•æ„Ÿè¯æ£€æµ‹æ¨¡å‹")
    parser.add_argument('--input-data', type=str, help='è¾“å…¥è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model-name', type=str, default='xlm-roberta-base', help='é¢„è®­ç»ƒæ¨¡å‹åç§°')
    parser.add_argument('--output-dir', type=str, help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    try:
        logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        processor = DataProcessor()
        
        # å¤„ç†æ•°æ®ï¼ˆå¦‚æœæä¾›äº†è¾“å…¥æ–‡ä»¶ï¼‰
        if args.input_data:
            if not Path(args.input_data).exists():
                raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.input_data}")
            
            logger.info(f"å¤„ç†è¾“å…¥æ•°æ®: {args.input_data}")
            train_file = config.data_config["train_file"]
            val_file = config.data_config["val_file"]
            test_file = config.data_config["test_file"]
            
            processor.process_and_split(args.input_data, train_file, val_file, test_file)
        
        # æ£€æŸ¥è®­ç»ƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        train_file = config.data_config["train_file"]
        val_file = config.data_config["val_file"]
        test_file = config.data_config["test_file"]
        
        if not Path(train_file).exists():
            raise FileNotFoundError(f"è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
        if not Path(val_file).exists():
            raise FileNotFoundError(f"éªŒè¯æ–‡ä»¶ä¸å­˜åœ¨: {val_file}")
        
        # åˆå§‹åŒ–è®­ç»ƒå™¨
        trainer = SensitiveWordTrainer(args.model_name)
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        trainer.load_model_and_tokenizer()
        
        # å‡†å¤‡æ•°æ®é›†
        train_dataset, val_dataset, test_dataset = trainer.prepare_datasets(
            train_file, val_file, test_file if Path(test_file).exists() else None
        )
        
        # è®¾ç½®è®­ç»ƒ
        output_dir = trainer.setup_training(train_dataset, val_dataset, args.output_dir)
        
        # è®­ç»ƒæ¨¡å‹
        results = trainer.train()
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        if test_dataset:
            test_results = trainer.evaluate_on_test_set(test_dataset)
            logger.info(f"æµ‹è¯•ç»“æœ: {test_results}")
        
        # ä¿å­˜æ¨¡å‹
        model_path = trainer.save_model()
        
        logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        logger.info(f"æ¨¡å‹ä¿å­˜åˆ°: {model_path}")
        logger.info(f"è®­ç»ƒæ—¥å¿—ä¿å­˜åˆ°: {output_dir}")
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åˆ°: {model_path}")
        print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—: {output_dir}")
        
        if test_dataset:
            test_accuracy = test_results.get('accuracy', 0)
            test_f1 = test_results.get('f1', 0)
            print(f"ğŸ¯ æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
            print(f"ğŸ“ˆ æµ‹è¯•F1åˆ†æ•°: {test_f1:.4f}")
        
    except Exception as e:
        logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()

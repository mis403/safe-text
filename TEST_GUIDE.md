# 🧪 交互式测试指南

本指南详细介绍如何使用交互式测试脚本来测试训练好的敏感词检测模型。

## 🚀 快速开始

### 1. 确保模型已训练

首先确保你已经训练了模型：

```bash
# 如果还没有训练模型
python train.py --input-data data/final_enhanced_training_data.csv
```

### 2. 选择测试方式

我们提供了两种测试方式：

#### 方式一：完整交互式测试（推荐）
```bash
python interactive_test.py
```

#### 方式二：快速测试
```bash
python quick_test.py
```

## 📋 详细功能说明

### 🔍 自动模型检测

两个测试脚本都会自动按优先级查找最新的模型：

1. **`ultimate_xlm_roberta_model/`** - 最新训练的模型（优先）
2. **`models/xlm_roberta_sensitive_filter/`** - 默认模型路径

脚本会显示找到的模型路径和训练时间，确保使用最新的模型。

### 🎯 完整交互式测试功能

运行 `python interactive_test.py` 后，你会看到以下选项：

#### 1. 交互式测试（推荐）
- **功能**: 逐个输入文本进行实时检测
- **适用**: 日常测试、验证特定文本
- **特色**: 详细的结果显示，包括置信度和概率分布

**使用示例**:
```
请输入文本: 这是一段正常的文本内容

📝 输入文本: 这是一段正常的文本内容
--------------------------------------------------
✅ 检测结果: 正常内容
🎯 置信度: 0.8234
📊 最终决策: normal

🤖 AI模型详情:
   预测类别: 0
   标签: 正常内容
   置信度: 0.8234
   概率分布:
     正常内容: 0.8234
     敏感内容: 0.1766
```

**特殊命令**:
- 输入 `quit` 或 `exit` 退出
- 输入 `batch` 切换到批量测试
- 输入 `info` 查看模型信息

#### 2. 批量测试
- **功能**: 一次输入多个文本进行批量检测
- **适用**: 测试大量文本、性能评估
- **特色**: 统计分析和汇总报告

**使用方法**:
1. 选择批量测试模式
2. 逐行输入文本（每行一个）
3. 输入空行结束输入
4. 查看批量处理结果和统计信息

**输出示例**:
```
📊 批量测试结果:
============================================================
 1. ✅ 这是第一段文本
     结果: 正常 (置信度: 0.8234)
 2. ✅ 这是第二段文本
     结果: 正常 (置信度: 0.7891)
 3. 🚨 这是敏感内容
     结果: 敏感 (置信度: 0.9123)
============================================================
📈 统计信息:
   总数: 3
   敏感: 1
   正常: 2
   敏感比例: 33.3%
```

#### 3. 预设测试用例
- **功能**: 运行预定义的测试用例
- **适用**: 模型基准测试、快速验证
- **特色**: 标准化测试，便于比较不同模型

预设测试包含10个常见的正常文本，用于验证模型的基本性能。

#### 4. 查看模型信息
显示当前加载模型的详细信息：
- 模型路径和训练时间
- 使用的计算设备（CPU/GPU/MPS）
- 模型配置参数
- 置信度阈值设置

### ⚡ 快速测试功能

运行 `python quick_test.py` 提供最简化的测试体验：

```bash
$ python quick_test.py

🚀 快速测试模式
✅ 使用模型: ultimate_xlm_roberta_model
✅ 模型加载成功!

💬 输入文本进行检测 (输入 'quit' 退出):
----------------------------------------

文本: 这是要测试的文本
✅ 结果: 正常内容 (置信度: 0.8234)

文本: quit
👋 再见!
```

## 🎨 输出格式说明

### 结果图标含义
- ✅ **正常内容** - 检测为非敏感内容
- 🚨 **敏感内容** - 检测为敏感内容

### 置信度解释
- **0.0-0.5**: 低置信度，模型不确定
- **0.5-0.7**: 中等置信度，有一定把握
- **0.7-0.9**: 高置信度，比较确定
- **0.9-1.0**: 极高置信度，非常确定

### 详细信息说明
- **预测类别**: 0=正常，1=敏感
- **标签**: 人类可读的分类标签
- **概率分布**: 每个类别的预测概率

## 🔧 高级用法

### 调整置信度阈值

如果需要调整敏感度，可以修改配置文件 `config/settings.py`：

```python
"inference": {
    "confidence_threshold": 0.5,  # 调整这个值
    # 0.3 = 更敏感（更多内容被标记为敏感）
    # 0.7 = 更保守（只有高置信度才标记为敏感）
}
```

### 批量测试文件

如果有大量测试数据，建议使用API接口的批量功能：

```python
import requests

texts = ["文本1", "文本2", "文本3", ...]
response = requests.post('http://localhost:8080/predict/batch',
                        json={'texts': texts})
```

## 🐛 常见问题

### Q: 提示"未找到训练好的模型"
**A**: 请先运行训练脚本：
```bash
python train.py --input-data your_training_data.csv
```

### Q: 模型加载很慢
**A**: 这是正常现象，首次加载需要时间。后续使用会更快。

### Q: 想测试特定的模型文件
**A**: 可以修改脚本中的模型路径，或者将模型文件放到标准位置。

### Q: 如何评估模型性能
**A**: 使用批量测试功能，准备标准测试集，观察准确率和置信度分布。

## 📊 性能建议

### 测试最佳实践
1. **多样化测试**: 测试不同类型、长度的文本
2. **边界测试**: 测试模糊、边界情况的文本
3. **批量测试**: 使用批量功能评估整体性能
4. **置信度分析**: 关注低置信度结果，可能需要更多训练数据

### 结果解读
- 高置信度的错误预测：可能需要更多相关训练数据
- 低置信度的预测：模型不确定，建议人工审核
- 批量测试的统计信息：用于评估模型整体表现

## 🎯 总结

交互式测试是验证模型效果的重要工具：

- **日常使用**: 选择快速测试模式
- **深入分析**: 选择完整交互式测试
- **性能评估**: 使用批量测试和预设用例
- **问题诊断**: 查看详细的概率分布信息

通过这些工具，你可以全面了解模型的表现，并根据需要进行调优。

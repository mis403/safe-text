#!/usr/bin/env python3
"""
Safe-Text APIæœåŠ¡å™¨
æ™ºèƒ½æ–‡æœ¬å®‰å…¨æ£€æµ‹ç³»ç»Ÿ - æ”¯æŒè®­ç»ƒå’Œæ¨ç†åŠŸèƒ½
"""

from flask import Flask, request, jsonify
import sys
import os
from pathlib import Path
import logging
from typing import Optional, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.models.inference import SensitiveWordInference
from src.models.trainer import SensitiveWordTrainer
from src.data import DataProcessor
from src.utils.model_finder import find_latest_model, find_all_models, get_model_info
from config.settings import config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# å…¨å±€æ¨ç†å¼•æ“
inference_engine: Optional[SensitiveWordInference] = None

# æ¨¡å‹æŸ¥æ‰¾åŠŸèƒ½å·²ç§»è‡³ src.utils.model_finder æ¨¡å—

def init_inference_engine():
    """åˆå§‹åŒ–æ¨ç†å¼•æ“"""
    global inference_engine
    if inference_engine is None:
        try:
            # æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
            model_path = find_latest_model()
            if not model_path:
                logger.warning("è®­ç»ƒå¥½çš„æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
                return False
            
            inference_engine = SensitiveWordInference(
                model_path=model_path,
                use_rules=False,  # åªä½¿ç”¨AIæ¨¡å‹
                use_ai=True
            )
            logger.info("æ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"æ¨ç†å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    return True

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    model_status = "loaded" if inference_engine else "not_loaded"
    return jsonify({
        "status": "healthy",
        "service": "safe-text",
        "model_status": model_status
    })

@app.route('/predict', methods=['POST'])
def predict_text():
    """æ•æ„Ÿè¯æ£€æµ‹æ¥å£"""
    if not inference_engine:
        return jsonify({
            "error": "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥æ¨¡å‹è·¯å¾„"
        }), 503
    
    try:
        # è·å–è¯·æ±‚æ•°æ®
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({
                "error": "ç¼ºå°‘å¿…éœ€çš„å‚æ•° 'text'"
            }), 400
        
        text = data['text']
        if not isinstance(text, str) or not text.strip():
            return jsonify({
                "error": "æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º"
            }), 400
        
        # è¿›è¡Œé¢„æµ‹
        result = inference_engine.predict_single(text)
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        response = {
            "text": text,
            "is_sensitive": result['is_sensitive'],
            "confidence": round(result['confidence'], 4),
            "label": "æ•æ„Ÿå†…å®¹" if result['is_sensitive'] else "æ­£å¸¸å†…å®¹"
        }
        
        # å¯é€‰ï¼šè¿”å›è¯¦ç»†çš„AIé¢„æµ‹ç»“æœ
        if data.get('include_details', False) and result.get('ai_result'):
            response['ai_details'] = result['ai_result']
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return jsonify({
            "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"
        }), 500

@app.route('/predict/batch', methods=['POST'])
def predict_batch():
    """æ‰¹é‡æ•æ„Ÿè¯æ£€æµ‹æ¥å£"""
    if not inference_engine:
        return jsonify({
            "error": "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹"
        }), 503
    
    try:
        data = request.get_json()
        if not data or 'texts' not in data:
            return jsonify({
                "error": "ç¼ºå°‘å¿…éœ€çš„å‚æ•° 'texts'"
            }), 400
        
        texts = data['texts']
        if not isinstance(texts, list) or len(texts) == 0:
            return jsonify({
                "error": "texts å¿…é¡»æ˜¯éç©ºæ•°ç»„"
            }), 400
        
        # é™åˆ¶æ‰¹é‡å¤§å°
        max_batch_size = 100
        if len(texts) > max_batch_size:
            return jsonify({
                "error": f"æ‰¹é‡å¤§å°ä¸èƒ½è¶…è¿‡ {max_batch_size}"
            }), 400
        
        # æ‰¹é‡é¢„æµ‹
        results = inference_engine.predict_batch(texts)
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        response = {
            "total": len(results),
            "results": []
        }
        
        for result in results:
            response["results"].append({
                "text": result['text'],
                "is_sensitive": result['is_sensitive'],
                "confidence": round(result['confidence'], 4),
                "label": "æ•æ„Ÿå†…å®¹" if result['is_sensitive'] else "æ­£å¸¸å†…å®¹"
            })
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"æ‰¹é‡é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return jsonify({
            "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"
        }), 500

@app.route('/train', methods=['POST'])
def train_model():
    """è®­ç»ƒæ¨¡å‹æ¥å£"""
    try:
        data = request.get_json() or {}
        
        # è·å–è®­ç»ƒå‚æ•°
        input_data = data.get('input_data')
        model_name = data.get('model_name', 'xlm-roberta-base')
        
        # æ£€æŸ¥è®­ç»ƒæ•°æ®
        if input_data:
            if not Path(input_data).exists():
                return jsonify({
                    "error": f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {input_data}"
                }), 400
        else:
            # æ£€æŸ¥é»˜è®¤è®­ç»ƒæ–‡ä»¶
            train_file = config.data_config["train_file"]
            if not Path(train_file).exists():
                return jsonify({
                    "error": "æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œè¯·æä¾› input_data å‚æ•°æˆ–ç¡®ä¿é»˜è®¤è®­ç»ƒæ–‡ä»¶å­˜åœ¨"
                }), 400
        
        logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        processor = DataProcessor()
        
        # å¤„ç†æ•°æ®ï¼ˆå¦‚æœæä¾›äº†è¾“å…¥æ–‡ä»¶ï¼‰
        if input_data:
            train_file = config.data_config["train_file"]
            val_file = config.data_config["val_file"]
            test_file = config.data_config["test_file"]
            
            processor.process_and_split(input_data, train_file, val_file, test_file)
        
        # åˆå§‹åŒ–è®­ç»ƒå™¨
        trainer = SensitiveWordTrainer(model_name)
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        trainer.load_model_and_tokenizer()
        
        # å‡†å¤‡æ•°æ®é›†
        train_file = config.data_config["train_file"]
        val_file = config.data_config["val_file"]
        test_file = config.data_config["test_file"]
        
        train_dataset, val_dataset, test_dataset = trainer.prepare_datasets(
            train_file, val_file, test_file if Path(test_file).exists() else None
        )
        
        # è®¾ç½®è®­ç»ƒ
        output_dir = trainer.setup_training(train_dataset, val_dataset)
        
        # è®­ç»ƒæ¨¡å‹
        results = trainer.train()
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        test_results = {}
        if test_dataset:
            test_results = trainer.evaluate_on_test_set(test_dataset)
        
        # ä¿å­˜æ¨¡å‹
        model_path = trainer.save_model()
        
        logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # é‡æ–°åˆå§‹åŒ–æ¨ç†å¼•æ“ä»¥ä½¿ç”¨æœ€æ–°è®­ç»ƒçš„æ¨¡å‹
        global inference_engine
        inference_engine = None
        if init_inference_engine():
            logger.info("æ¨ç†å¼•æ“å·²æ›´æ–°ä¸ºæœ€æ–°è®­ç»ƒçš„æ¨¡å‹")
        else:
            logger.warning("æ¨ç†å¼•æ“æ›´æ–°å¤±è´¥")
        
        return jsonify({
            "message": "æ¨¡å‹è®­ç»ƒå®Œæˆ",
            "model_path": str(model_path),
            "output_dir": str(output_dir),
            "training_results": {
                "train_loss": results.get('train_loss', 0),
                "eval_loss": results.get('eval_loss', 0)
            },
            "test_results": test_results
        })
        
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return jsonify({
            "error": f"è®­ç»ƒå¤±è´¥: {str(e)}"
        }), 500

@app.route('/model/status', methods=['GET'])
def model_status():
    """è·å–æ¨¡å‹çŠ¶æ€"""
    current_model_path = find_latest_model()
    
    # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹è·¯å¾„
    model_paths = {
        "current_model": {
            "path": current_model_path or "æœªæ‰¾åˆ°",
            "exists": current_model_path is not None,
            "is_active": True
        },
        "checkpoint_model": {
            "path": "ultimate_xlm_roberta_model/checkpoint-100",
            "exists": Path("ultimate_xlm_roberta_model/checkpoint-100").exists()
        },
        "ultimate_model": {
            "path": "ultimate_xlm_roberta_model",
            "exists": Path("ultimate_xlm_roberta_model").exists()
        },
        "default_model": {
            "path": str(Path(config.paths["models_dir"]) / "xlm_roberta_sensitive_filter"),
            "exists": (Path(config.paths["models_dir"]) / "xlm_roberta_sensitive_filter").exists()
        }
    }
    
    status = {
        "inference_engine_loaded": inference_engine is not None,
        "model_paths": model_paths,
        "training_data": {
            "train_file": {
                "path": config.data_config["train_file"],
                "exists": Path(config.data_config["train_file"]).exists()
            },
            "val_file": {
                "path": config.data_config["val_file"],
                "exists": Path(config.data_config["val_file"]).exists()
            },
            "test_file": {
                "path": config.data_config["test_file"],
                "exists": Path(config.data_config["test_file"]).exists()
            }
        }
    }
    
    return jsonify(status)

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨æ•æ„Ÿè¯æ£€æµ‹APIæœåŠ¡å™¨...")
    
    # å°è¯•åˆå§‹åŒ–æ¨ç†å¼•æ“
    if init_inference_engine():
        print("âœ… æ¨ç†å¼•æ“åŠ è½½æˆåŠŸ")
    else:
        print("âš ï¸  æ¨ç†å¼•æ“æœªåŠ è½½ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
    
    print("ğŸ“¡ APIæœåŠ¡å™¨è¿è¡Œåœ¨: http://localhost:8080")
    print("\nå¯ç”¨æ¥å£:")
    print("  GET  /health          - å¥åº·æ£€æŸ¥")
    print("  GET  /model/status    - æ¨¡å‹çŠ¶æ€")
    print("  POST /predict         - å•æ–‡æœ¬é¢„æµ‹")
    print("  POST /predict/batch   - æ‰¹é‡æ–‡æœ¬é¢„æµ‹")
    print("  POST /train           - è®­ç»ƒæ¨¡å‹")
    
    # å¯åŠ¨æœåŠ¡
    app.run(
        host='0.0.0.0',
        port=8080,
        debug=False
    )

#!/usr/bin/env python3
"""
Safe-Text APIæœåŠ¡å™¨
æ™ºèƒ½æ–‡æœ¬å®‰å…¨æ£€æµ‹ç³»ç»Ÿ - æ”¯æŒè®­ç»ƒå’Œæ¨ç†åŠŸèƒ½
"""

from flask import Flask, request, jsonify
import sys
import os
import argparse
from pathlib import Path
import logging
from typing import Optional, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.models.inference import SensitiveWordInference
from src.models.trainer import SensitiveWordTrainer
from src.data import DataProcessor
from src.utils.model_finder import find_latest_model, find_all_models, get_model_info
from config.settings import config
from config.simple_config import load_training_config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# å…¨å±€æ¨ç†å¼•æ“ (æ¯ä¸ªå·¥ä½œè¿›ç¨‹ç‹¬ç«‹å®ä¾‹)
inference_engine: Optional[SensitiveWordInference] = None

# è¿›ç¨‹åˆå§‹åŒ–æ ‡è®°
_process_initialized = False

# åŠ è½½å®‰å…¨é…ç½®
try:
    training_config = load_training_config()
    security_config = training_config.get('security', {})
    API_TOKEN = security_config.get('api_token', 'safe-text-api-2025-default')
    TOKEN_REQUIRED = security_config.get('token_required', True)
except Exception as e:
    logger.warning(f"æ— æ³•åŠ è½½å®‰å…¨é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
    API_TOKEN = 'safe-text-api-2025-default'
    TOKEN_REQUIRED = True

# æ— éœ€è®¤è¯çš„ç«¯ç‚¹åˆ—è¡¨
EXEMPT_ENDPOINTS = ['/auth/info']

@app.before_request
def authenticate_request():
    """å…¨å±€è®¤è¯ä¸­é—´ä»¶ - åœ¨æ¯ä¸ªè¯·æ±‚å‰æ‰§è¡Œ"""
    # å¦‚æœè®¤è¯è¢«ç¦ç”¨ï¼Œç›´æ¥é€šè¿‡
    if not TOKEN_REQUIRED:
        return None
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯å…è®¤è¯ç«¯ç‚¹
    if request.endpoint and any(request.path.startswith(exempt) for exempt in EXEMPT_ENDPOINTS):
        return None
    
    # è·å–Authorizationå¤´
    auth_header = request.headers.get('Authorization')
    
    if not auth_header:
        return jsonify({
            "error": "ç¼ºå°‘Authorizationå¤´",
            "message": "è¯·åœ¨è¯·æ±‚å¤´ä¸­åŒ…å«: Authorization: Bearer <token>"
        }), 401
    
    # æ£€æŸ¥Beareræ ¼å¼
    if not auth_header.startswith('Bearer '):
        return jsonify({
            "error": "æ— æ•ˆçš„Authorizationæ ¼å¼",
            "message": "æ ¼å¼åº”ä¸º: Authorization: Bearer <token>"
        }), 401
    
    # æå–token
    token = auth_header[7:]  # ç§»é™¤ "Bearer " å‰ç¼€
    
    # éªŒè¯token
    if token != API_TOKEN:
        return jsonify({
            "error": "æ— æ•ˆçš„è®¿é—®ä»¤ç‰Œ",
            "message": "è¯·æä¾›æœ‰æ•ˆçš„API Token"
        }), 403
    
    # è®¤è¯æˆåŠŸï¼Œç»§ç»­å¤„ç†è¯·æ±‚
    return None

# æ¨¡å‹æŸ¥æ‰¾åŠŸèƒ½å·²ç§»è‡³ src.utils.model_finder æ¨¡å—

def init_inference_engine():
    """åˆå§‹åŒ–æ¨ç†å¼•æ“ - æ”¯æŒå¤šè¿›ç¨‹ç‹¬ç«‹åˆå§‹åŒ–"""
    global inference_engine, _process_initialized
    
    # é˜²æ­¢åŒä¸€è¿›ç¨‹é‡å¤åˆå§‹åŒ–
    if _process_initialized and inference_engine is not None:
        return True
    
    try:
        # è·å–å½“å‰è¿›ç¨‹ID
        import os
        process_id = os.getpid()
        logger.info(f"ğŸ” è¿›ç¨‹ {process_id}: æŸ¥æ‰¾æœ€æ–°è®­ç»ƒçš„æ¨¡å‹...")
        
        # æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
        model_path = find_latest_model()
        if not model_path:
            logger.warning(f"è¿›ç¨‹ {process_id}: è®­ç»ƒå¥½çš„æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return False
        
        # åˆå§‹åŒ–æ¨ç†å¼•æ“
        logger.info(f"ğŸ¤– è¿›ç¨‹ {process_id}: åŠ è½½æ¨¡å‹ {model_path}")
        inference_engine = SensitiveWordInference(
            model_path=model_path,
            use_rules=False,  # åªä½¿ç”¨AIæ¨¡å‹
            use_ai=True
        )
        _process_initialized = True
        
        logger.info(f"âœ… è¿›ç¨‹ {process_id}: æ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"âŒ è¿›ç¨‹ {process_id}: æ¨ç†å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    model_status = "loaded" if inference_engine else "not_loaded"
    return jsonify({
        "status": "healthy",
        "service": "safe-text",
        "model_status": model_status
    })

@app.route('/predict', methods=['POST'])
def predict_text():
    """æ•æ„Ÿè¯æ£€æµ‹æ¥å£"""
    # ç¡®ä¿æ¨ç†å¼•æ“å·²åˆå§‹åŒ–ï¼ˆå¤šè¿›ç¨‹å®‰å…¨ï¼‰
    if not init_inference_engine():
        return jsonify({
            "error": "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥æ¨¡å‹è·¯å¾„"
        }), 503
    
    try:
        # è·å–è¯·æ±‚æ•°æ®
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({
                "error": "ç¼ºå°‘å¿…éœ€çš„å‚æ•° 'text'"
            }), 400
        
        text = data['text']
        if not isinstance(text, str) or not text.strip():
            return jsonify({
                "error": "æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º"
            }), 400
        
        # è¿›è¡Œé¢„æµ‹
        result = inference_engine.predict_single(text)
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        response = {
            "text": text,
            "is_sensitive": result['is_sensitive'],
            "confidence": round(result['confidence'], 4),
            "label": "æ•æ„Ÿå†…å®¹" if result['is_sensitive'] else "æ­£å¸¸å†…å®¹"
        }
        
        # å¯é€‰ï¼šè¿”å›è¯¦ç»†çš„AIé¢„æµ‹ç»“æœ
        if data.get('include_details', False) and result.get('ai_result'):
            response['ai_details'] = result['ai_result']
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return jsonify({
            "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"
        }), 500

@app.route('/predict/batch', methods=['POST'])
def predict_batch():
    """æ‰¹é‡æ•æ„Ÿè¯æ£€æµ‹æ¥å£"""
    # ç¡®ä¿æ¨ç†å¼•æ“å·²åˆå§‹åŒ–ï¼ˆå¤šè¿›ç¨‹å®‰å…¨ï¼‰
    if not init_inference_engine():
        return jsonify({
            "error": "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹"
        }), 503
    
    try:
        data = request.get_json()
        if not data or 'texts' not in data:
            return jsonify({
                "error": "ç¼ºå°‘å¿…éœ€çš„å‚æ•° 'texts'"
            }), 400
        
        texts = data['texts']
        if not isinstance(texts, list) or len(texts) == 0:
            return jsonify({
                "error": "texts å¿…é¡»æ˜¯éç©ºæ•°ç»„"
            }), 400
        
        # é™åˆ¶æ‰¹é‡å¤§å°
        max_batch_size = 100
        if len(texts) > max_batch_size:
            return jsonify({
                "error": f"æ‰¹é‡å¤§å°ä¸èƒ½è¶…è¿‡ {max_batch_size}"
            }), 400
        
        # æ‰¹é‡é¢„æµ‹
        results = inference_engine.predict_batch(texts)
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        response = {
            "total": len(results),
            "results": []
        }
        
        for result in results:
            response["results"].append({
                "text": result['text'],
                "is_sensitive": result['is_sensitive'],
                "confidence": round(result['confidence'], 4),
                "label": "æ•æ„Ÿå†…å®¹" if result['is_sensitive'] else "æ­£å¸¸å†…å®¹"
            })
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"æ‰¹é‡é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return jsonify({
            "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"
        }), 500

@app.route('/train', methods=['POST'])
def train_model():
    """è®­ç»ƒæ¨¡å‹æ¥å£"""
    try:
        data = request.get_json() or {}
        
        # è·å–è®­ç»ƒå‚æ•°
        input_data = data.get('input_data')
        model_name = data.get('model_name', 'xlm-roberta-base')
        
        # æ£€æŸ¥è®­ç»ƒæ•°æ®
        if input_data:
            if not Path(input_data).exists():
                return jsonify({
                    "error": f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {input_data}"
                }), 400
        else:
            # æ£€æŸ¥é»˜è®¤è®­ç»ƒæ–‡ä»¶
            train_file = config.data_config["train_file"]
            if not Path(train_file).exists():
                return jsonify({
                    "error": "æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œè¯·æä¾› input_data å‚æ•°æˆ–ç¡®ä¿é»˜è®¤è®­ç»ƒæ–‡ä»¶å­˜åœ¨"
                }), 400
        
        logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        processor = DataProcessor()
        
        # å¤„ç†æ•°æ®ï¼ˆå¦‚æœæä¾›äº†è¾“å…¥æ–‡ä»¶ï¼‰
        if input_data:
            train_file = config.data_config["train_file"]
            val_file = config.data_config["val_file"]
            test_file = config.data_config["test_file"]
            
            processor.process_and_split(input_data, train_file, val_file, test_file)
        
        # åˆå§‹åŒ–è®­ç»ƒå™¨
        trainer = SensitiveWordTrainer(model_name)
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        trainer.load_model_and_tokenizer()
        
        # å‡†å¤‡æ•°æ®é›†
        train_file = config.data_config["train_file"]
        val_file = config.data_config["val_file"]
        test_file = config.data_config["test_file"]
        
        train_dataset, val_dataset, test_dataset = trainer.prepare_datasets(
            train_file, val_file, test_file if Path(test_file).exists() else None
        )
        
        # è®¾ç½®è®­ç»ƒ
        output_dir = trainer.setup_training(train_dataset, val_dataset)
        
        # è®­ç»ƒæ¨¡å‹
        results = trainer.train()
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        test_results = {}
        if test_dataset:
            test_results = trainer.evaluate_on_test_set(test_dataset)
        
        # ä¿å­˜æ¨¡å‹
        model_path = trainer.save_model()
        
        logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # é‡æ–°åˆå§‹åŒ–æ¨ç†å¼•æ“ä»¥ä½¿ç”¨æœ€æ–°è®­ç»ƒçš„æ¨¡å‹
        global inference_engine
        inference_engine = None
        if init_inference_engine():
            logger.info("æ¨ç†å¼•æ“å·²æ›´æ–°ä¸ºæœ€æ–°è®­ç»ƒçš„æ¨¡å‹")
        else:
            logger.warning("æ¨ç†å¼•æ“æ›´æ–°å¤±è´¥")
        
        return jsonify({
            "message": "æ¨¡å‹è®­ç»ƒå®Œæˆ",
            "model_path": str(model_path),
            "output_dir": str(output_dir),
            "training_results": {
                "train_loss": results.get('train_loss', 0),
                "eval_loss": results.get('eval_loss', 0)
            },
            "test_results": test_results
        })
        
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return jsonify({
            "error": f"è®­ç»ƒå¤±è´¥: {str(e)}"
        }), 500

@app.route('/model/status', methods=['GET'])
def model_status():
    """è·å–æ¨¡å‹çŠ¶æ€"""
    current_model_path = find_latest_model()
    
    # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹è·¯å¾„
    model_paths = {
        "current_model": {
            "path": current_model_path or "æœªæ‰¾åˆ°",
            "exists": current_model_path is not None,
            "is_active": True
        },
        "checkpoint_model": {
            "path": "ultimate_xlm_roberta_model/checkpoint-100",
            "exists": Path("ultimate_xlm_roberta_model/checkpoint-100").exists()
        },
        "ultimate_model": {
            "path": "ultimate_xlm_roberta_model",
            "exists": Path("ultimate_xlm_roberta_model").exists()
        },
        "default_model": {
            "path": str(Path(config.paths["models_dir"]) / "xlm_roberta_sensitive_filter"),
            "exists": (Path(config.paths["models_dir"]) / "xlm_roberta_sensitive_filter").exists()
        }
    }
    
    status = {
        "inference_engine_loaded": inference_engine is not None,
        "model_paths": model_paths,
        "training_data": {
            "train_file": {
                "path": config.data_config["train_file"],
                "exists": Path(config.data_config["train_file"]).exists()
            },
            "val_file": {
                "path": config.data_config["val_file"],
                "exists": Path(config.data_config["val_file"]).exists()
            },
            "test_file": {
                "path": config.data_config["test_file"],
                "exists": Path(config.data_config["test_file"]).exists()
            }
        }
    }
    
    return jsonify(status)

@app.route('/auth/info', methods=['GET'])
def auth_info():
    """è·å–è®¤è¯ä¿¡æ¯ï¼ˆæ— éœ€Tokenï¼‰"""
    return jsonify({
        "token_required": TOKEN_REQUIRED,
        "auth_method": "Bearer Token",
        "header_format": "Authorization: Bearer <token>",
        "message": "è¯·è”ç³»ç®¡ç†å‘˜è·å–API Token" if TOKEN_REQUIRED else "å½“å‰æ— éœ€è®¤è¯"
    })

if __name__ == '__main__':
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='Safe-Text APIæœåŠ¡å™¨')
    parser.add_argument('--host', default='0.0.0.0', help='æœåŠ¡å™¨ä¸»æœºåœ°å€ (é»˜è®¤: 0.0.0.0)')
    parser.add_argument('--port', type=int, default=9900, help='æœåŠ¡å™¨ç«¯å£å· (é»˜è®¤: 9900)')
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    args = parser.parse_args()
    
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    host = args.host
    port = args.port
    debug = args.debug
    
    print("ğŸš€ å¯åŠ¨æ•æ„Ÿè¯æ£€æµ‹APIæœåŠ¡å™¨...")
    
    # æ˜¾ç¤ºå®‰å…¨é…ç½®
    if TOKEN_REQUIRED:
        print("ğŸ” APIå®‰å…¨è®¤è¯å·²å¯ç”¨")
        print(f"ğŸ”‘ API Token: {API_TOKEN}")
        print("ğŸ“‹ è¯·æ±‚å¤´æ ¼å¼: Authorization: Bearer <token>")
    else:
        print("âš ï¸  APIå®‰å…¨è®¤è¯å·²ç¦ç”¨")
    
    # å°è¯•åˆå§‹åŒ–æ¨ç†å¼•æ“
    if init_inference_engine():
        print("âœ… æ¨ç†å¼•æ“åŠ è½½æˆåŠŸ")
    else:
        print("âš ï¸  æ¨ç†å¼•æ“æœªåŠ è½½ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
    
    print(f"ğŸ“¡ APIæœåŠ¡å™¨è¿è¡Œåœ¨: http://{host}:{port}")
    print("\nå¯ç”¨æ¥å£:")
    print("  GET  /auth/info       - è®¤è¯ä¿¡æ¯ (æ— éœ€Token)")
    print("  GET  /health          - å¥åº·æ£€æŸ¥")
    print("  GET  /model/status    - æ¨¡å‹çŠ¶æ€")
    print("  POST /predict         - å•æ–‡æœ¬é¢„æµ‹")
    print("  POST /predict/batch   - æ‰¹é‡æ–‡æœ¬é¢„æµ‹")
    print("  POST /train           - è®­ç»ƒæ¨¡å‹")
    
    # å¯åŠ¨æœåŠ¡
    app.run(
        host=host,
        port=port,
        debug=debug
    )
